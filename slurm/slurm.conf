# Slurm Configuration File for HPC Energy Model Project
# This configuration is designed for a virtualized environment in Proxmox
# with energy monitoring capabilities

# CLUSTER IDENTIFICATION
ClusterName=hpc-energy-cluster
ControlMachine=slurm-controller
ControlAddr=slurm-controller

# AUTHENTICATION
AuthType=auth/munge
AuthInfo=/var/run/munge/munge.socket.2

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# RESOURCE LIMITS
DefMemPerCPU=1024
MaxMemPerCPU=8192
DefCpuPerGPU=4
MaxCpuPerGPU=8

# JOB SUBMISSION
JobSubmitPlugins=lua
JobCompType=jobcomp/script
JobCompLoc=/opt/slurm/scripts/job_completion.sh

# ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=slurm-controller
AccountingStoragePort=6819
AccountingStorageUser=slurm
AccountingStoragePass=slurmdbpass
AccountingStorageTRES=cpu,mem,energy,gres/gpu

# JOB ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30
JobAcctGatherParams=UsePss,NoOverMemoryKill

# PROCESS TRACKING
ProctrackType=proctrack/cgroup
TaskPlugin=task/cgroup,task/affinity

# CGROUP CONFIGURATION
CgroupAutomount=yes
CgroupMountpoint=/sys/fs/cgroup
ConstrainCores=yes
ConstrainRAMSpace=yes
ConstrainSwapSpace=yes
ConstrainDevices=yes

# ENERGY ACCOUNTING (Custom Plugin)
AcctGatherEnergyType=acct_gather_energy/rapl
AcctGatherNodeFreq=30
AcctGatherEnergyFreq=30

# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmSchedLogFile=/var/log/slurm/sched.log
SlurmSchedLogLevel=1

# TIMEOUTS
MessageTimeout=10
TCPTimeout=2
InactiveLimit=0
KillWait=30
MinJobAge=300
WaitTime=0

# SCHEDULING PARAMETERS
SchedulerTimeSlice=30
SchedulerParameters=default_queue_depth=100,max_switch_wait=300,partition_job_depth=500
FastSchedule=1
PriorityType=priority/multifactor
PriorityDecayHalfLife=1-0
PriorityCalcPeriod=5
PriorityFavorSmall=NO
PriorityMaxAge=7-0
PriorityUsageResetPeriod=NONE
PriorityWeightAge=1000
PriorityWeightFairshare=10000
PriorityWeightJobSize=1000
PriorityWeightPartition=10000
PriorityWeightQOS=2000

# HEALTH CHECK
HealthCheckProgram=/opt/slurm/scripts/health_check.sh
HealthCheckInterval=300
HealthCheckNodeState=ANY

# POWER MANAGEMENT
SuspendProgram=/opt/slurm/scripts/suspend_node.sh
ResumeProgram=/opt/slurm/scripts/resume_node.sh
SuspendTimeout=60
ResumeTimeout=300
SuspendExcNodes=slurm-controller
SuspendExcParts=debug
BatchStartTimeout=10
CompleteWait=0
KillOnBadExit=1
RebootProgram=/opt/slurm/scripts/reboot_node.sh

# TOPOLOGY (for energy-aware scheduling)
TopologyPlugin=topology/tree

# PREEMPTION
PreemptType=preempt/qos
PreemptMode=REQUEUE

# PLUGINS FOR ENERGY MONITORING
PluginDir=/usr/lib64/slurm
PlugStackConfig=/etc/slurm/plugstack.conf

# COMMUNICATION
SlurmctldPort=6817
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/slurmd
StateSaveLocation=/var/spool/slurm/slurmctld
SwitchType=switch/none
MpiDefault=none

# COMPUTE NODES CONFIGURATION
# Node definitions for AWS instances
# Head Node: c5.xlarge - 4 vCPUs, 8 GiB RAM (virtualized)
# Compute Nodes: c5.metal - 96 logical processors, 192 GiB RAM (bare metal)
# Up to 3.4-3.6 GHz sustained all-core turbo, up to 10 Gbps network performance

# AWS ParallelCluster Compute Nodes - c5.metal instances
# Dynamic node naming pattern: compute-dy-[queue]-[index]
NodeName=compute-dy-compute-[1-10] \
    CPUs=96 \
    RealMemory=196608 \
    Sockets=2 \
    CoresPerSocket=24 \
    ThreadsPerCore=2 \
    State=CLOUD \
    Features=cloud,intel_xeon,thermal_monitoring,energy_monitoring \
    Gres=energy:1 \
    Weight=1

# Alternative static node definition for testing/debugging
NodeName=compute-st-01 \
    CPUs=4 \
    RealMemory=7680 \
    Sockets=1 \
    CoresPerSocket=4 \
    ThreadsPerCore=1 \
    State=UNKNOWN \
    NodeAddr=compute-st-01 \
    NodeHostName=compute-st-01 \
    Features=static,intel_xeon,thermal_monitoring,energy_monitoring \
    Gres=energy:1

# PARTITION DEFINITIONS
# Head node (c5.xlarge): 4 CPUs, Compute nodes (c5.metal): 96 CPUs

# Debug partition for testing
PartitionName=debug \
    Nodes=compute-st-01 \
    Default=YES \
    MaxTime=30:00 \
    State=UP \
    DefaultTime=10:00 \
    MaxNodes=1 \
    MaxCPUsPerNode=4 \
    PriorityJobFactor=10000 \
    PriorityTier=10000

# Main compute partition for AWS ParallelCluster
PartitionName=compute \
    Nodes=compute-dy-compute-[1-10] \
    Default=NO \
    MaxTime=48:00:00 \
    State=UP \
    DefaultTime=04:00:00 \
    MaxNodes=10 \
    MaxCPUsPerNode=96 \
    PriorityJobFactor=5000 \
    PriorityTier=5000

# CPU-intensive partition
PartitionName=cpu \
    Nodes=compute-dy-compute-[1-10] \
    Default=NO \
    MaxTime=48:00:00 \
    State=UP \
    DefaultTime=04:00:00 \
    MaxNodes=10 \
    MaxCPUsPerNode=96 \
    PriorityJobFactor=4000 \
    PriorityTier=4000

# Energy-efficient partition (optimized scheduling)
PartitionName=energy_efficient \
    Nodes=compute-dy-compute-[1-5] \
    Default=NO \
    MaxTime=72:00:00 \
    State=UP \
    DefaultTime=08:00:00 \
    MaxNodes=5 \
    MaxCPUsPerNode=96 \
    PriorityJobFactor=2000 \
    PriorityTier=2000

# Long-running jobs partition
PartitionName=long \
    Nodes=compute-dy-compute-[1-10] \
    Default=NO \
    MaxTime=168:00:00 \
    State=UP \
    DefaultTime=24:00:00 \
    MaxNodes=10 \
    MaxCPUsPerNode=96 \
    PriorityJobFactor=1000 \
    PriorityTier=1000

# QUALITY OF SERVICE (QoS) DEFINITIONS
# These would be defined in the Slurm database, but listed here for reference
# - normal: Default QoS
# - high: High priority jobs
# - low: Low priority, energy-efficient jobs
# - debug: Debug and testing jobs
# - gpu: GPU-accelerated jobs
# - energy_aware: Jobs with energy consumption constraints

# ENERGY-AWARE SCHEDULING CONFIGURATION
# Custom parameters for energy optimization
SchedulerParameters=bf_max_job_test=1000,bf_max_job_user=50,bf_continue,energy_aware_scheduling=yes

# GRES (Generic Resources) Configuration
# Energy monitoring as a consumable resource
GresTypes=gpu,energy

# EPILOG AND PROLOG SCRIPTS
# Scripts to run before and after jobs for energy monitoring
Prolog=/opt/slurm/scripts/job_prolog.sh
Epilog=/opt/slurm/scripts/job_epilog.sh
PrologSlurmctld=/opt/slurm/scripts/prolog_slurmctld.sh
EpilogSlurmctld=/opt/slurm/scripts/epilog_slurmctld.sh

# TASK EPILOG/PROLOG
TaskProlog=/opt/slurm/scripts/task_prolog.sh
TaskEpilog=/opt/slurm/scripts/task_epilog.sh

# BURST BUFFER (if needed for I/O intensive workloads)
# BurstBufferType=burst_buffer/generic

# FEDERATION (for multi-cluster setups)
# FederationParameters=fed_display

# LICENSES (for software licensing)
# Licenses=matlab:10,ansys:5

# MAIL CONFIGURATION
MailProg=/usr/bin/mail

# SLURM DATABASE DAEMON CONFIGURATION
# This would typically be in a separate slurmdbd.conf file
# but included here for completeness

# CUSTOM ENERGY MONITORING PARAMETERS
# These are custom parameters for the energy monitoring system
JobSubmitPlugins=lua
LuaScriptPath=/opt/slurm/scripts/job_submit.lua

# CGROUP CONFIGURATION FILE REFERENCE
# Detailed cgroup configuration would be in /etc/slurm/cgroup.conf

# TOPOLOGY CONFIGURATION
# Detailed topology would be in /etc/slurm/topology.conf
# for energy-aware node placement

# PLUGSTACK CONFIGURATION
# Additional plugins for energy monitoring would be configured
# in /etc/slurm/plugstack.conf

# RETURN CODES
ReturnToService=1
SlurmdUser=slurm
SlurmUser=slurm

# TEMPORARY DIRECTORIES
TmpFS=/tmp
SlurmdSpoolDir=/var/spool/slurm/slurmd
StateSaveLocation=/var/spool/slurm/slurmctld

# NETWORK CONFIGURATION
TreeWidth=50

# ADVANCED SCHEDULING
EnforcePartLimits=ALL
OverTimeLimit=0
UnkillableStepTimeout=60
VSizeFactor=0

# DEBUGGING
DebugFlags=NO_CONF_HASH,SelectType

# ENERGY ACCOUNTING TRES
# Track energy consumption as a TRES (Trackable Resource)
AccountingStorageTRES=cpu,mem,energy,node,billing,gres/gpu

# JOB CONTAINER SUPPORT (if using containers)
# JobContainerType=job_container/tmpfs

# SITE-SPECIFIC PARAMETERS
# Add any site-specific configuration here

# END OF CONFIGURATION