name: Performance Testing and Benchmarking

on:
  push:
    branches: [ main ]
    paths:
      - 'workloads/**'
      - 'analysis/**'
      - 'api/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'workloads/**'
      - 'analysis/**'
      - 'api/**'
  schedule:
    # Run performance tests weekly
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - cpu
        - io
        - mixed
        - api
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '60'

jobs:
  setup-performance-env:
    runs-on: ubuntu-latest
    name: Setup Performance Testing Environment
    outputs:
      python-version: ${{ steps.setup.outputs.python-version }}
      benchmark-type: ${{ steps.setup.outputs.benchmark-type }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup environment variables
      id: setup
      run: |
        echo "python-version=3.9" >> $GITHUB_OUTPUT
        echo "benchmark-type=${{ github.event.inputs.benchmark_type || 'all' }}" >> $GITHUB_OUTPUT
        echo "duration=${{ github.event.inputs.duration || '60' }}" >> $GITHUB_OUTPUT

  cpu-benchmark:
    runs-on: ubuntu-latest
    name: CPU Intensive Benchmark
    needs: setup-performance-env
    if: needs.setup-performance-env.outputs.benchmark-type == 'all' || needs.setup-performance-env.outputs.benchmark-type == 'cpu'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ needs.setup-performance-env.outputs.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install psutil numpy scipy matplotlib pandas
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements_ml_performance.txt ]; then pip install -r requirements_ml_performance.txt; fi
    
    - name: Run CPU benchmark
      run: |
        cd workloads/cpu-intensive
        
        echo "=== CPU Benchmark Test ==="
        echo "System Information:"
        python -c "import psutil; print(f'CPU cores: {psutil.cpu_count()}')"
        python -c "import psutil; print(f'Memory: {psutil.virtual_memory().total / (1024**3):.2f} GB')"
        
        # Run benchmark with performance monitoring
        echo "Running CPU intensive benchmark..."
        timeout ${{ github.event.inputs.duration || '60' }}s python cpu_benchmark.py --duration 50 --output-format json > cpu_results.json || true
        
        if [ -f cpu_results.json ]; then
          echo "CPU Benchmark Results:"
          cat cpu_results.json
        else
          echo "Running basic CPU test..."
          python -c "
import time
import json
start = time.time()
result = sum(i*i for i in range(1000000))
end = time.time()
print(json.dumps({'duration': end-start, 'result': result, 'test': 'basic_cpu'}))
" > cpu_results.json
          cat cpu_results.json
        fi
    
    - name: Analyze CPU performance
      run: |
        cd workloads/cpu-intensive
        
        echo "=== CPU Performance Analysis ==="
        python -c "
import json
import sys

try:
    with open('cpu_results.json', 'r') as f:
        results = json.load(f)
    
    print(f'CPU Test Duration: {results.get(\"duration\", \"N/A\")} seconds')
    print(f'Operations per second: {results.get(\"ops_per_sec\", \"N/A\")}')
    print(f'CPU utilization: {results.get(\"cpu_utilization\", \"N/A\")}')
    
    # Performance thresholds
    if 'duration' in results and results['duration'] > 120:
        print('⚠ Warning: CPU benchmark took longer than expected')
        sys.exit(1)
    
    print('✓ CPU benchmark completed successfully')
except Exception as e:
    print(f'Error analyzing results: {e}')
    sys.exit(1)
"
    
    - name: Upload CPU benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: cpu-benchmark-results
        path: workloads/cpu-intensive/cpu_results.json
        retention-days: 30

  io-benchmark:
    runs-on: ubuntu-latest
    name: I/O Intensive Benchmark
    needs: setup-performance-env
    if: needs.setup-performance-env.outputs.benchmark-type == 'all' || needs.setup-performance-env.outputs.benchmark-type == 'io'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ needs.setup-performance-env.outputs.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install psutil numpy
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Run I/O benchmark
      run: |
        cd workloads/io-intensive
        
        echo "=== I/O Benchmark Test ==="
        echo "Disk Information:"
        df -h .
        
        # Run I/O benchmark
        echo "Running I/O intensive benchmark..."
        timeout ${{ github.event.inputs.duration || '60' }}s python io_benchmark.py --duration 50 --output-format json > io_results.json || true
        
        if [ -f io_results.json ]; then
          echo "I/O Benchmark Results:"
          cat io_results.json
        else
          echo "Running basic I/O test..."
          python -c "
import time
import json
import os

start = time.time()
test_file = 'test_io.tmp'
with open(test_file, 'w') as f:
    for i in range(10000):
        f.write(f'Line {i}\\n')

with open(test_file, 'r') as f:
    lines = f.readlines()

os.remove(test_file)
end = time.time()

print(json.dumps({'duration': end-start, 'lines_written': 10000, 'lines_read': len(lines), 'test': 'basic_io'}))
" > io_results.json
          cat io_results.json
        fi
    
    - name: Analyze I/O performance
      run: |
        cd workloads/io-intensive
        
        echo "=== I/O Performance Analysis ==="
        python -c "
import json
import sys

try:
    with open('io_results.json', 'r') as f:
        results = json.load(f)
    
    print(f'I/O Test Duration: {results.get(\"duration\", \"N/A\")} seconds')
    print(f'Read throughput: {results.get(\"read_throughput\", \"N/A\")} MB/s')
    print(f'Write throughput: {results.get(\"write_throughput\", \"N/A\")} MB/s')
    
    # Performance thresholds
    if 'duration' in results and results['duration'] > 120:
        print('⚠ Warning: I/O benchmark took longer than expected')
        sys.exit(1)
    
    print('✓ I/O benchmark completed successfully')
except Exception as e:
    print(f'Error analyzing results: {e}')
    sys.exit(1)
"
    
    - name: Upload I/O benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: io-benchmark-results
        path: workloads/io-intensive/io_results.json
        retention-days: 30

  mixed-benchmark:
    runs-on: ubuntu-latest
    name: Mixed Workload Benchmark
    needs: setup-performance-env
    if: needs.setup-performance-env.outputs.benchmark-type == 'all' || needs.setup-performance-env.outputs.benchmark-type == 'mixed'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ needs.setup-performance-env.outputs.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install psutil numpy scipy
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Run mixed workload benchmark
      run: |
        cd workloads/mixed
        
        echo "=== Mixed Workload Benchmark Test ==="
        
        # Run mixed benchmark
        echo "Running mixed workload benchmark..."
        timeout ${{ github.event.inputs.duration || '60' }}s python mixed_benchmark.py --duration 50 --output-format json > mixed_results.json || true
        
        if [ -f mixed_results.json ]; then
          echo "Mixed Benchmark Results:"
          cat mixed_results.json
        else
          echo "Running basic mixed test..."
          python -c "
import time
import json
import os
import threading

def cpu_task():
    return sum(i*i for i in range(100000))

def io_task():
    with open('temp_mixed.tmp', 'w') as f:
        for i in range(1000):
            f.write(f'Data {i}\\n')
    os.remove('temp_mixed.tmp')

start = time.time()
threads = []
for _ in range(2):
    t1 = threading.Thread(target=cpu_task)
    t2 = threading.Thread(target=io_task)
    threads.extend([t1, t2])
    t1.start()
    t2.start()

for t in threads:
    t.join()

end = time.time()
print(json.dumps({'duration': end-start, 'threads': len(threads), 'test': 'basic_mixed'}))
" > mixed_results.json
          cat mixed_results.json
        fi
    
    - name: Analyze mixed workload performance
      run: |
        cd workloads/mixed
        
        echo "=== Mixed Workload Performance Analysis ==="
        python -c "
import json
import sys

try:
    with open('mixed_results.json', 'r') as f:
        results = json.load(f)
    
    print(f'Mixed Test Duration: {results.get(\"duration\", \"N/A\")} seconds')
    print(f'CPU efficiency: {results.get(\"cpu_efficiency\", \"N/A\")}')
    print(f'I/O efficiency: {results.get(\"io_efficiency\", \"N/A\")}')
    
    # Performance thresholds
    if 'duration' in results and results['duration'] > 120:
        print('⚠ Warning: Mixed benchmark took longer than expected')
        sys.exit(1)
    
    print('✓ Mixed benchmark completed successfully')
except Exception as e:
    print(f'Error analyzing results: {e}')
    sys.exit(1)
"
    
    - name: Upload mixed benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: mixed-benchmark-results
        path: workloads/mixed/mixed_results.json
        retention-days: 30

  api-performance-test:
    runs-on: ubuntu-latest
    name: API Performance Test
    needs: setup-performance-env
    if: needs.setup-performance-env.outputs.benchmark-type == 'all' || needs.setup-performance-env.outputs.benchmark-type == 'api'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ needs.setup-performance-env.outputs.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests locust pytest-benchmark
        if [ -f api/requirements.txt ]; then pip install -r api/requirements.txt; fi
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Start API server
      run: |
        cd api
        echo "Starting API server for performance testing..."
        
        # Make entrypoint executable and start server in background
        chmod +x entrypoint.sh
        ./entrypoint.sh &
        API_PID=$!
        echo "API_PID=$API_PID" >> $GITHUB_ENV
        
        # Wait for API to start
        sleep 10
        
        # Test if API is responding
        curl -f http://localhost:8000/health || echo "API health check failed"
    
    - name: Run API load test
      run: |
        echo "=== API Performance Test ==="
        
        # Create a simple load test script
        cat > api_load_test.py << 'EOF'
import requests
import time
import json
import threading
from concurrent.futures import ThreadPoolExecutor

def make_request(url):
    try:
        start = time.time()
        response = requests.get(url, timeout=5)
        end = time.time()
        return {
            'status_code': response.status_code,
            'response_time': end - start,
            'success': response.status_code == 200
        }
    except Exception as e:
        return {
            'status_code': 0,
            'response_time': 0,
            'success': False,
            'error': str(e)
        }

def run_load_test(url, num_requests=100, num_threads=10):
    results = []
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(make_request, url) for _ in range(num_requests)]
        for future in futures:
            results.append(future.result())
    
    return results

if __name__ == '__main__':
    # Test endpoints
    base_url = 'http://localhost:8000'
    endpoints = ['/health', '/api/predict', '/api/status']
    
    all_results = {}
    
    for endpoint in endpoints:
        url = base_url + endpoint
        print(f'Testing {url}...')
        
        try:
            results = run_load_test(url, num_requests=50, num_threads=5)
            
            successful_requests = [r for r in results if r['success']]
            response_times = [r['response_time'] for r in successful_requests]
            
            if response_times:
                avg_response_time = sum(response_times) / len(response_times)
                max_response_time = max(response_times)
                min_response_time = min(response_times)
            else:
                avg_response_time = max_response_time = min_response_time = 0
            
            all_results[endpoint] = {
                'total_requests': len(results),
                'successful_requests': len(successful_requests),
                'success_rate': len(successful_requests) / len(results) * 100,
                'avg_response_time': avg_response_time,
                'max_response_time': max_response_time,
                'min_response_time': min_response_time
            }
            
        except Exception as e:
            all_results[endpoint] = {'error': str(e)}
    
    # Save results
    with open('api_performance_results.json', 'w') as f:
        json.dump(all_results, f, indent=2)
    
    # Print summary
    for endpoint, results in all_results.items():
        print(f'\n{endpoint}:')
        if 'error' in results:
            print(f'  Error: {results["error"]}')
        else:
            print(f'  Success rate: {results["success_rate"]:.1f}%')
            print(f'  Avg response time: {results["avg_response_time"]:.3f}s')
            print(f'  Max response time: {results["max_response_time"]:.3f}s')
EOF
        
        python api_load_test.py
    
    - name: Analyze API performance
      run: |
        echo "=== API Performance Analysis ==="
        
        if [ -f api_performance_results.json ]; then
          python -c "
import json
import sys

with open('api_performance_results.json', 'r') as f:
    results = json.load(f)

print('API Performance Summary:')
for endpoint, data in results.items():
    print(f'\\n{endpoint}:')
    if 'error' in data:
        print(f'  ❌ Error: {data[\"error\"]}')
    else:
        success_rate = data.get('success_rate', 0)
        avg_time = data.get('avg_response_time', 0)
        
        if success_rate < 95:
            print(f'  ⚠ Low success rate: {success_rate:.1f}%')
        else:
            print(f'  ✓ Success rate: {success_rate:.1f}%')
        
        if avg_time > 2.0:
            print(f'  ⚠ Slow response time: {avg_time:.3f}s')
        else:
            print(f'  ✓ Response time: {avg_time:.3f}s')
"
        else
          echo "No API performance results found"
        fi
    
    - name: Stop API server
      if: always()
      run: |
        if [ -n "$API_PID" ]; then
          kill $API_PID || true
        fi
        pkill -f "python.*energy_prediction_api" || true
    
    - name: Upload API performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: api-performance-results
        path: api_performance_results.json
        retention-days: 30

  performance-summary:
    runs-on: ubuntu-latest
    name: Performance Summary
    needs: [cpu-benchmark, io-benchmark, mixed-benchmark, api-performance-test]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate performance report
      run: |
        echo "# Performance Test Summary" > performance_report.md
        echo "Generated on: $(date)" >> performance_report.md
        echo "" >> performance_report.md
        
        # CPU Benchmark Summary
        if [ -f cpu-benchmark-results/cpu_results.json ]; then
          echo "## CPU Benchmark" >> performance_report.md
          python -c "
import json
with open('cpu-benchmark-results/cpu_results.json', 'r') as f:
    data = json.load(f)
print(f'- Duration: {data.get(\"duration\", \"N/A\")} seconds')
print(f'- Operations/sec: {data.get(\"ops_per_sec\", \"N/A\")}')
" >> performance_report.md
          echo "" >> performance_report.md
        fi
        
        # I/O Benchmark Summary
        if [ -f io-benchmark-results/io_results.json ]; then
          echo "## I/O Benchmark" >> performance_report.md
          python -c "
import json
with open('io-benchmark-results/io_results.json', 'r') as f:
    data = json.load(f)
print(f'- Duration: {data.get(\"duration\", \"N/A\")} seconds')
print(f'- Read throughput: {data.get(\"read_throughput\", \"N/A\")} MB/s')
print(f'- Write throughput: {data.get(\"write_throughput\", \"N/A\")} MB/s')
" >> performance_report.md
          echo "" >> performance_report.md
        fi
        
        # Mixed Benchmark Summary
        if [ -f mixed-benchmark-results/mixed_results.json ]; then
          echo "## Mixed Workload Benchmark" >> performance_report.md
          python -c "
import json
with open('mixed-benchmark-results/mixed_results.json', 'r') as f:
    data = json.load(f)
print(f'- Duration: {data.get(\"duration\", \"N/A\")} seconds')
print(f'- CPU efficiency: {data.get(\"cpu_efficiency\", \"N/A\")}')
print(f'- I/O efficiency: {data.get(\"io_efficiency\", \"N/A\")}')
" >> performance_report.md
          echo "" >> performance_report.md
        fi
        
        # API Performance Summary
        if [ -f api-performance-results/api_performance_results.json ]; then
          echo "## API Performance" >> performance_report.md
          python -c "
import json
with open('api-performance-results/api_performance_results.json', 'r') as f:
    data = json.load(f)
for endpoint, results in data.items():
    print(f'### {endpoint}')
    if 'error' not in results:
        print(f'- Success rate: {results.get(\"success_rate\", 0):.1f}%')
        print(f'- Avg response time: {results.get(\"avg_response_time\", 0):.3f}s')
    print('')
" >> performance_report.md
        fi
        
        echo "Performance report generated:"
        cat performance_report.md
    
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: performance_report.md
        retention-days: 90